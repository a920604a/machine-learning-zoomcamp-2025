{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42ed3e6c-81dd-4b53-86f7-a5258954ee5f",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "To be able to use this model, we need to know the name of the input and output nodes.\n",
    "\n",
    "What's the name of the output:\n",
    "\n",
    "- `output`\n",
    "- `sigmoid`\n",
    "- `softmax`\n",
    "- `prediction`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7ac3d44-9f05-4fe8-9ce1-5c4ffb4c8753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting onnx\n",
      "  Downloading onnx-1.20.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/site-packages (from onnx) (2.3.5)\n",
      "Collecting protobuf>=4.25.1 (from onnx)\n",
      "  Downloading protobuf-6.33.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.11/site-packages (from onnx) (4.15.0)\n",
      "Collecting ml_dtypes>=0.5.0 (from onnx)\n",
      "  Downloading ml_dtypes-0.5.4-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Downloading onnx-1.20.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (18.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.5.4-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-6.33.1-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.2/323.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf, ml_dtypes, onnx\n",
      "Successfully installed ml_dtypes-0.5.4 onnx-1.20.0 protobuf-6.33.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7db9eb2-ca9d-4ff7-88a3-3c72aacec124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      "input\n",
      "\n",
      "Outputs:\n",
      "output\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "# 讀取 ONNX 模型\n",
    "model = onnx.load(\"hair_classifier_v1.onnx\")\n",
    "\n",
    "# 列出所有輸入節點\n",
    "print(\"Inputs:\")\n",
    "for i in model.graph.input:\n",
    "    print(i.name)\n",
    "\n",
    "# 列出所有輸出節點\n",
    "print(\"\\nOutputs:\")\n",
    "for o in model.graph.output:\n",
    "    print(o.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e78fa69-909c-4795-9ef5-955916a703dc",
   "metadata": {},
   "source": [
    "### Preparing the image\n",
    "You'll need some code for downloading and resizing images. You can use this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cedd4e7-b25b-47dd-bf63-3ad7e23c40da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from urllib import request\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def download_image(url):\n",
    "    with request.urlopen(url) as resp:\n",
    "        buffer = resp.read()\n",
    "    stream = BytesIO(buffer)\n",
    "    img = Image.open(stream)\n",
    "    return img\n",
    "\n",
    "\n",
    "def prepare_image(img, target_size):\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    img = img.resize(target_size, Image.NEAREST)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "684af1a9-65e7-43ec-8b0b-6b806b1174f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in /usr/local/lib/python3.11/site-packages (12.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5d3b3e-77e2-4609-905e-f2939fe0603a",
   "metadata": {},
   "source": [
    "## Question 2: Target size\n",
    "Let's download and resize this image:\n",
    "\n",
    "`https://habrastorage.org/webt/yf/_d/ok/yf_dokzqy3vcritme8ggnzqlvwa.jpeg`\n",
    "\n",
    "Based on the previous homework, what should be the target size for the image?\n",
    "\n",
    "- 64x64\n",
    "- 128x128\n",
    "- 200x200\n",
    "- 256x256\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2066ec5-83f3-41d5-8268-cf41e571b59a",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Now we need to turn the image into numpy array and pre-process it.\n",
    "\n",
    "Tip: Check the previous homework. What was the pre-processing we did there?\n",
    "\n",
    "After the pre-processing, what's the value in the first pixel, the R channel?\n",
    "\n",
    "- -10.73\n",
    "- -1.073\n",
    "- 1.073\n",
    "- 10.73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de75e3d0-4783-4ea8-a2ff-9de7d7de081d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = download_image('https://habrastorage.org/webt/yf/_d/ok/yf_dokzqy3vcritme8ggnzqlvwa.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56b7a544-fb12-451e-9dcc-fae8a4c002c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0732939435925546\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Resize\n",
    "img = img.resize((200, 200), Image.NEAREST)\n",
    "\n",
    "# 轉 numpy array 並縮放\n",
    "img_arr = np.array(img).astype(np.float32) / 255.0\n",
    "\n",
    "# ImageNet normalization\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std  = np.array([0.229, 0.224, 0.225])\n",
    "img_arr = (img_arr - mean) / std\n",
    "\n",
    "# 查看第一個像素 R channel\n",
    "first_pixel_r = img_arr[0,0,0]\n",
    "print(first_pixel_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a015ff0a-29e1-4aba-8bec-4812be439b61",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "Now let's apply this model to this image. What's the output of the model\n",
    "- 0.09\n",
    "- 0.49\n",
    "- 0.69\n",
    "- 0.89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9cfb33af-eb58-410c-b442-e0098eb8693b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting onnxruntime\n",
      "  Downloading onnxruntime-1.23.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
      "Collecting coloredlogs (from onnxruntime)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime)\n",
      "  Downloading flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/site-packages (from onnxruntime) (2.3.5)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/site-packages (from onnxruntime) (25.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/site-packages (from onnxruntime) (6.33.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/site-packages (from onnxruntime) (1.14.0)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/site-packages (from sympy->onnxruntime) (1.3.0)\n",
      "Downloading onnxruntime-1.23.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: flatbuffers, humanfriendly, coloredlogs, onnxruntime\n",
      "Successfully installed coloredlogs-15.0.1 flatbuffers-25.9.23 humanfriendly-10.0 onnxruntime-1.23.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73afb214-fe81-4366-9626-1fa85fc2cd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09156641]]\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "# 定義 preprocessing\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((200, 200)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "img_tensor = preprocess(img)  # [3, 200, 200]\n",
    "img_tensor = img_tensor.unsqueeze(0).numpy()  # [1, 3, 200, 200]，onnxruntime 需要 numpy\n",
    "\n",
    "# 載入模型\n",
    "session = ort.InferenceSession(\"hair_classifier_v1.onnx\")\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_name = session.get_outputs()[0].name\n",
    "\n",
    "# 推論\n",
    "pred = session.run([output_name], {input_name: img_tensor})[0]\n",
    "print(pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31175899-eed0-4abd-9db5-9f20c104313b",
   "metadata": {},
   "source": [
    "## uestion 5\n",
    "Download the base image `agrigorev/model-2025-hairstyle:v1`. You can do it with docker pull.\n",
    "\n",
    "So what's the size of this base image?\n",
    "\n",
    "- 88 Mb\n",
    "- 208 Mb\n",
    "- 608 Mb\n",
    "- 1208 Mb\n",
    "You can get this information when running docker images - it'll be in the \"SIZE\" column."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
