{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e395b17-4bb2-4033-ad10-08dc7d8325de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-12-02 06:28:23--  https://github.com/SVizor42/ML_Zoomcamp/releases/download/straight-curly-data/data.zip\n",
      "Resolving github.com (github.com)... 20.27.177.113\n",
      "Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://release-assets.githubusercontent.com/github-production-release-asset/405934815/e712cf72-f851-44e0-9c05-e711624af985?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-12-02T07%3A10%3A14Z&rscd=attachment%3B+filename%3Ddata.zip&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-12-02T06%3A10%3A08Z&ske=2025-12-02T07%3A10%3A14Z&sks=b&skv=2018-11-09&sig=2iLuipeGBW5uqu3iDwmmlnuLbO5F6Sz3GCc3oU30pig%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NDY1ODcwMywibmJmIjoxNzY0NjU2OTAzLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ._UOfz3FcV6GW_uPiu4KSKQLWEkwJm323VhXxcdgzNCo&response-content-disposition=attachment%3B%20filename%3Ddata.zip&response-content-type=application%2Foctet-stream [following]\n",
      "--2025-12-02 06:28:23--  https://release-assets.githubusercontent.com/github-production-release-asset/405934815/e712cf72-f851-44e0-9c05-e711624af985?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-12-02T07%3A10%3A14Z&rscd=attachment%3B+filename%3Ddata.zip&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-12-02T06%3A10%3A08Z&ske=2025-12-02T07%3A10%3A14Z&sks=b&skv=2018-11-09&sig=2iLuipeGBW5uqu3iDwmmlnuLbO5F6Sz3GCc3oU30pig%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NDY1ODcwMywibmJmIjoxNzY0NjU2OTAzLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ._UOfz3FcV6GW_uPiu4KSKQLWEkwJm323VhXxcdgzNCo&response-content-disposition=attachment%3B%20filename%3Ddata.zip&response-content-type=application%2Foctet-stream\n",
      "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
      "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 102516572 (98M) [application/octet-stream]\n",
      "Saving to: ‘data.zip’\n",
      "\n",
      "data.zip            100%[===================>]  97.77M  10.3MB/s    in 8.0s    \n",
      "\n",
      "2025-12-02 06:28:31 (12.1 MB/s) - ‘data.zip’ saved [102516572/102516572]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/SVizor42/ML_Zoomcamp/releases/download/straight-curly-data/data.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "731966e5-28be-4d62-95a7-b27957f94d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637388cd-1c50-40e9-89eb-f53ad402eade",
   "metadata": {},
   "source": [
    "## CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2185036e-0b15-4f18-b283-36f3369c24af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class HairCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HairCNN, self).__init__()\n",
    "\n",
    "        # Conv layer: 32 filters, kernel 3x3, padding=0, stride=1\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=3,\n",
    "            out_channels=32,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=0\n",
    "        )\n",
    "\n",
    "        # MaxPool 2x2\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Fully connected layers\n",
    "        # Input shape: (3, 200, 200)\n",
    "        # After conv (no padding): -> (32, 198, 198)\n",
    "        # After 2×2 maxpool: -> (32, 99, 99)\n",
    "        flatten_dim = 32 * 99 * 99\n",
    "\n",
    "        self.fc1 = nn.Linear(flatten_dim, 64)  # 64 neurons\n",
    "        self.fc2 = nn.Linear(64, 1)             # output layer (1 neuron)\n",
    "        self.sigmoid = nn.Sigmoid()             # binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)   # or x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        # x = self.sigmoid(self.fc2(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8692759c-1a2f-419d-9f9c-62c8f784632e",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b745c8c7-ca39-46e2-8680-3e076927f165",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "Which loss function you will use?\n",
    "\n",
    "- `nn.MSELoss()`\n",
    "- `nn.BCEWithLogitsLoss()`\n",
    "- `nn.CrossEntropyLoss()`\n",
    "- `nn.CosineEmbeddingLoss()`\n",
    "\n",
    "(Multiple answered can be correct, so pick any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d184043f-9aac-4852-8304-042709393314",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HairCNN()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.002, momentum=0.8)\n",
    "criterion = nn.BCEWithLogitsLoss()  # 二元分類對應 sigmoid 輸出\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48659040-7c00-46cd-adea-675c3964a8e0",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "What's the total number of parameters of the model? You can use torchsummary or count manually.\n",
    "\n",
    "In PyTorch, you can find the total number of parameters using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e4fc401-7144-4cf8-93e9-9dfcb604b557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 198, 198]             896\n",
      "         MaxPool2d-2           [-1, 32, 99, 99]               0\n",
      "            Linear-3                   [-1, 64]      20,072,512\n",
      "            Linear-4                    [-1, 1]              65\n",
      "================================================================\n",
      "Total params: 20,073,473\n",
      "Trainable params: 20,073,473\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.46\n",
      "Forward/backward pass size (MB): 11.96\n",
      "Params size (MB): 76.57\n",
      "Estimated Total Size (MB): 89.00\n",
      "----------------------------------------------------------------\n",
      "Total parameters: 20073473\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Using torchsummary (install with: pip install torchsummary)\n",
    "from torchsummary import summary\n",
    "summary(model, input_size=(3, 200, 200))\n",
    "\n",
    "# Option 2: Manual counting\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1cf27d-584f-4560-9af9-ebc09e9ab96b",
   "metadata": {},
   "source": [
    "## Generators and Training\n",
    "For the next two questions, use the following transformation for both train and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e550cba5-f324-4bd0-ad7d-992fe319bda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((200, 200)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ) # ImageNet normalization\n",
    "])\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((200, 200)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ) # ImageNet normalization\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b297c4-d05e-451b-8266-d486b8e96f94",
   "metadata": {},
   "source": [
    "We don't need to do any additional pre-processing for the images.\n",
    "\n",
    "- Use `batch_size`=20\n",
    "- Use `shuffle=True` for both training, but `False` for test.\n",
    "\n",
    "Now fit the model.\n",
    "\n",
    "You can use this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "423ddb3d-8378-47e5-9ecf-759dfda59ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = ImageFolder(\"data/train\", transform=train_transforms)\n",
    "validation_dataset  = ImageFolder(\"data/test\",  transform=test_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "validation_loader  = DataLoader(test_dataset,  batch_size=20, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb921aaf-471d-4560-ba05-82ac8a21fc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# 自動選擇 GPU（如果有）或 CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# 將模型也搬到 device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf9bbcad-b64c-4bca-88a3-6e4375cfe481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.5333, Acc: 0.7388, Val Loss: 0.6053, Val Acc: 0.6567\n",
      "Epoch 2/10, Loss: 0.4731, Acc: 0.7775, Val Loss: 0.6417, Val Acc: 0.6667\n",
      "Epoch 3/10, Loss: 0.4803, Acc: 0.7588, Val Loss: 0.8099, Val Acc: 0.6119\n",
      "Epoch 4/10, Loss: 0.3998, Acc: 0.8313, Val Loss: 0.6123, Val Acc: 0.6716\n",
      "Epoch 5/10, Loss: 0.2956, Acc: 0.8688, Val Loss: 0.7451, Val Acc: 0.7114\n",
      "Epoch 6/10, Loss: 0.3051, Acc: 0.8638, Val Loss: 0.7088, Val Acc: 0.6866\n",
      "Epoch 7/10, Loss: 0.2172, Acc: 0.9025, Val Loss: 0.9213, Val Acc: 0.6716\n",
      "Epoch 8/10, Loss: 0.3192, Acc: 0.8675, Val Loss: 0.7105, Val Acc: 0.7114\n",
      "Epoch 9/10, Loss: 0.1625, Acc: 0.9413, Val Loss: 0.7524, Val Acc: 0.7363\n",
      "Epoch 10/10, Loss: 0.1446, Acc: 0.9463, Val Loss: 0.8719, Val Acc: 0.7164\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "history = {'acc': [], 'loss': [], 'val_acc': [], 'val_loss': []}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        labels = labels.float().unsqueeze(1) # Ensure labels are float and have shape (batch_size, 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        # For binary classification with BCEWithLogitsLoss, apply sigmoid to outputs before thresholding for accuracy\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    epoch_acc = correct_train / total_train\n",
    "    history['loss'].append(epoch_loss)\n",
    "    history['acc'].append(epoch_acc)\n",
    "\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in validation_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            labels = labels.float().unsqueeze(1)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_running_loss += loss.item() * images.size(0)\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "    val_epoch_loss = val_running_loss / len(validation_dataset)\n",
    "    val_epoch_acc = correct_val / total_val\n",
    "    history['val_loss'].append(val_epoch_loss)\n",
    "    history['val_acc'].append(val_epoch_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "          f\"Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}, \"\n",
    "          f\"Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f55ebd1-8e33-4559-a144-64f238914ffa",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "What is the median of training accuracy for all the epochs for this model?\n",
    "- 0.05\n",
    "- 0.12\n",
    "- 0.40\n",
    "- 0.84\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afa0e55c-ea43-4377-8cf7-1567eeb234e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8656250000000001\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "median_acc = np.median(history['acc'])\n",
    "print(median_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4580af-97c1-43bf-8b83-5c220358e45a",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "What is the standard deviation of training loss for all the epochs for this model?\n",
    "- 0.007\n",
    "- 0.078\n",
    "- 0.171\n",
    "- 1.710"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fdda5bf-1453-43af-b156-b56ba174973e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12893830517029975\n"
     ]
    }
   ],
   "source": [
    "std_loss = np.std(history['loss'])\n",
    "print(std_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b68a0a-74a4-4ab7-a649-01e0376fa944",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "For the next two questions, we'll generate more data using data augmentations.\n",
    "\n",
    "Add the following augmentations to your training data generator:\n",
    "\n",
    "```python\n",
    "transforms.RandomRotation(50),\n",
    "transforms.RandomResizedCrop(200, scale=(0.9, 1.0), ratio=(0.9, 1.1)),\n",
    "transforms.RandomHorizontalFlip(),\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2815fe1-6e5f-4ded-9b45-bbebfd478895",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomRotation(50),  # 隨機旋轉 ±50 度\n",
    "    transforms.RandomResizedCrop(200, scale=(0.9, 1.0), ratio=(0.9, 1.1)),  # 隨機裁切並 resize\n",
    "    transforms.RandomHorizontalFlip(),  # 隨機水平翻轉\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# test transforms 不需要 augmentation\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((200, 200)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ebedf0f9-ba51-4dc5-9364-e0d9e1c15856",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = ImageFolder(\"data/train\", transform=train_transforms)\n",
    "validation_dataset  = ImageFolder(\"data/test\",  transform=test_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "validation_loader  = DataLoader(test_dataset,  batch_size=20, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9bfeaab7-2ac0-4823-87ae-662afee2936f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.6396, Acc: 0.6538, Val Loss: 0.7832, Val Acc: 0.6517\n",
      "Epoch 2/10, Loss: 0.5501, Acc: 0.7250, Val Loss: 0.6757, Val Acc: 0.7363\n",
      "Epoch 3/10, Loss: 0.5448, Acc: 0.7300, Val Loss: 0.6059, Val Acc: 0.6866\n",
      "Epoch 4/10, Loss: 0.5009, Acc: 0.7562, Val Loss: 0.5618, Val Acc: 0.7214\n",
      "Epoch 5/10, Loss: 0.5022, Acc: 0.7388, Val Loss: 0.5541, Val Acc: 0.7264\n",
      "Epoch 6/10, Loss: 0.4952, Acc: 0.7375, Val Loss: 0.5530, Val Acc: 0.7264\n",
      "Epoch 7/10, Loss: 0.4512, Acc: 0.7762, Val Loss: 0.5583, Val Acc: 0.7313\n",
      "Epoch 8/10, Loss: 0.4493, Acc: 0.7800, Val Loss: 0.6339, Val Acc: 0.6517\n",
      "Epoch 9/10, Loss: 0.4458, Acc: 0.8013, Val Loss: 0.5465, Val Acc: 0.7214\n",
      "Epoch 10/10, Loss: 0.4610, Acc: 0.7700, Val Loss: 0.5261, Val Acc: 0.7065\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "history = {'acc': [], 'loss': [], 'val_acc': [], 'val_loss': []}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        labels = labels.float().unsqueeze(1) # Ensure labels are float and have shape (batch_size, 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        # For binary classification with BCEWithLogitsLoss, apply sigmoid to outputs before thresholding for accuracy\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    epoch_acc = correct_train / total_train\n",
    "    history['loss'].append(epoch_loss)\n",
    "    history['acc'].append(epoch_acc)\n",
    "\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in validation_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            labels = labels.float().unsqueeze(1)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_running_loss += loss.item() * images.size(0)\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "    val_epoch_loss = val_running_loss / len(validation_dataset)\n",
    "    val_epoch_acc = correct_val / total_val\n",
    "    history['val_loss'].append(val_epoch_loss)\n",
    "    history['val_acc'].append(val_epoch_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "          f\"Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}, \"\n",
    "          f\"Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07819f98-bb7f-4093-a82d-3df6858698e5",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "Let's train our model for 10 more epochs using the same code as previously.\n",
    "\n",
    "Note: make sure you don't re-create the model. we want to continue training the model we already started training.\n",
    "\n",
    "What is the mean of test loss for all the epochs for the model trained with augmentations?\n",
    "\n",
    "- 0.008\n",
    "- 0.08\n",
    "- 0.88\n",
    "- 8.88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b78977a2-d174-4de8-84f0-1d52ab09f102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Test Loss (all epochs): 0.5998579801834045\n"
     ]
    }
   ],
   "source": [
    "mean_test_loss = np.mean(history['val_loss'])\n",
    "print(\"Mean Test Loss (all epochs):\", mean_test_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8132c7-69f6-4eb9-98c3-e74d8d505b44",
   "metadata": {},
   "source": [
    "\n",
    "### Question 6\n",
    "What's the average of test accuracy for the last 5 epochs (from 6 to 10) for the model trained with augmentations?\n",
    "\n",
    "- 0.08\n",
    "- 0.28\n",
    "- 0.68\n",
    "- 0.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6876909c-05d8-42e8-a755-31f48a9406e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Accuracy (last 5 epochs): 0.7074626865671643\n"
     ]
    }
   ],
   "source": [
    "avg_last5_test_acc = np.mean(history['val_acc'][-5:])\n",
    "print(\"Average Test Accuracy (last 5 epochs):\", avg_last5_test_acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
